{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# character rnn workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "designed to give some hands on exposure to a chracter rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filesystem and data downloading libraries\n",
    "import os # for filesystem operations\n",
    "import requests # for interacting with webpages to get data\n",
    "from zipfile import ZipFile # unzip in python to keep it all in one place\\\n",
    "import glob # easy file matching\n",
    "\n",
    "# numerical libraries\n",
    "import random\n",
    "import numpy as np # linear algebra library\n",
    "\n",
    "# text processing libraries\n",
    "import string\n",
    "import unicodedata\n",
    "\n",
    "import time\n",
    "\n",
    "# deep learning library\n",
    "import torch # mostly for tensors\n",
    "import torch.nn as nn # the Neural Networks module from torch. nn by convention\n",
    "\n",
    "# plotting library\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# premade\n",
    "url = 'https://download.pytorch.org/tutorial/data.zip'\n",
    "\n",
    "# data download and file managment\n",
    "if not os.path.isdir('data'):\n",
    "    os.mkdir('data')\n",
    "if not os.path.isfile('data/name_data.zip'):\n",
    "    r = requests.get(url) # download the data\n",
    "    with open('data/name_data.zip', 'wb') as f:\n",
    "        f.write(r.content) # save the data to a file\n",
    "\n",
    "# extract the data\n",
    "with ZipFile('data/name_data.zip', 'r') as data_zip:\n",
    "   # Extract all the contents of the data zip file in to the data directory\n",
    "   data_zip.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at the folders and the file contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 ['Khoury\\n', 'Nahas\\n', 'Daher\\n', 'Gerges\\n', 'Nazari\\n', 'Maalouf\\n', 'Gerges\\n', 'Naifeh\\n', 'Guirguis\\n', 'Baba\\n']\n"
     ]
    }
   ],
   "source": [
    "# print out the data directory contents\n",
    "# print out some of the file contents\n",
    "os.listdir('data/names')\n",
    "with open('data/names/Arabic.txt') as f:\n",
    "    lines = [line for line in f]\n",
    "print(len(lines), lines[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/names/Czech.txt',\n",
       " 'data/names/German.txt',\n",
       " 'data/names/Arabic.txt',\n",
       " 'data/names/Japanese.txt',\n",
       " 'data/names/Chinese.txt',\n",
       " 'data/names/Vietnamese.txt',\n",
       " 'data/names/Russian.txt',\n",
       " 'data/names/French.txt',\n",
       " 'data/names/Irish.txt',\n",
       " 'data/names/English.txt',\n",
       " 'data/names/Spanish.txt',\n",
       " 'data/names/Greek.txt',\n",
       " 'data/names/Italian.txt',\n",
       " 'data/names/Portuguese.txt',\n",
       " 'data/names/Scottish.txt',\n",
       " 'data/names/Dutch.txt',\n",
       " 'data/names/Korean.txt',\n",
       " 'data/names/Polish.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set all language files as a list\n",
    "fnames = glob.glob('data/names/*.txt')\n",
    "fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string cleaner helper functions\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "def clean_names(name):\n",
    "    return unicode_to_ascii(name.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .,;'\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set global variables of the total character set\n",
    "all_letters = string.ascii_letters + \" .,;'\"\n",
    "n_letters = len(all_letters)\n",
    "n_letters, all_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 ['Czech', 'German', 'Arabic', 'Japanese', 'Chinese', 'Vietnamese', 'Russian', 'French', 'Irish', 'English', 'Spanish', 'Greek', 'Italian', 'Portuguese', 'Scottish', 'Dutch', 'Korean', 'Polish']\n"
     ]
    }
   ],
   "source": [
    "# load the language files into a list of languages and a {language:names} dict\n",
    "all_languages = [] # list of language names\n",
    "language_names = {} # dict of key = language, value = list of all names in that language\n",
    "for fname in fnames:\n",
    "    with open(fname, 'r') as f:\n",
    "        language = os.path.basename(fname)[:-4]\n",
    "        names = [clean_names(line) for line in f]\n",
    "        all_languages.append(language)\n",
    "        language_names[language] = names\n",
    "n_languages = len(all_languages)\n",
    "print(n_languages, all_languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data as tensors (vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# livecode helper functions\n",
    "def character_to_tensor(c):\n",
    "    t = torch.zeros(1, n_letters) # size of tensor (1,n)\n",
    "    t[0, all_letters.find(c)] = 1 # set to one at index of character\n",
    "    return t\n",
    "\n",
    "def word_to_tensor(word):\n",
    "    w = torch.zeros(len(word), 1, n_letters) # size of tensor (w, 1, n)\n",
    "    for i, c in enumerate(word):\n",
    "        w[i,0,all_letters.find(c)] = 1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try out some letter/word to tensor examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 57])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_to_tensor('a').size()#, character_to_tensor('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 57])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_tensor('abc').size()#, word_to_tensor('abc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def random_choice(l):\n",
    "    \"\"\"random selection from list\"\"\"\n",
    "    item = l[random.randint(0,len(l)-1)]\n",
    "    return item \n",
    "    \n",
    "def random_language_name(language=None):\n",
    "    \"\"\"returns all the information for a random language-name pair\n",
    "    \n",
    "    args:\n",
    "        language = if None, select a random language\n",
    "    \"\"\"\n",
    "    if not(language):\n",
    "        language = random_choice(all_languages)\n",
    "    name = random_choice(language_names[language])\n",
    "    \n",
    "    language_index = all_languages.index(language)\n",
    "    language_tensor = torch.tensor([language_index], dtype=torch.long)\n",
    "    name_tensor = word_to_tensor(name)\n",
    "    return language, name, language_tensor, name_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('German',\n",
       " 'Kunkel',\n",
       " tensor([1]),\n",
       " tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_language_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "language:  Irish tensor([8]) name:  Desmond torch.Size([7, 1, 57])\n",
      "language:  Russian tensor([6]) name:  Zhadanovsky torch.Size([11, 1, 57])\n",
      "language:  Vietnamese tensor([5]) name:  Luong torch.Size([5, 1, 57])\n",
      "language:  Japanese tensor([3]) name:  Okimoto torch.Size([7, 1, 57])\n",
      "language:  Czech tensor([0]) name:  Egr torch.Size([3, 1, 57])\n",
      "language:  Korean tensor([16]) name:  Rhee torch.Size([4, 1, 57])\n",
      "language:  Irish tensor([8]) name:  Seaghdha torch.Size([8, 1, 57])\n",
      "language:  Portuguese tensor([13]) name:  Santos torch.Size([6, 1, 57])\n",
      "language:  Polish tensor([17]) name:  Wojewodzki torch.Size([10, 1, 57])\n",
      "language:  German tensor([1]) name:  Groel torch.Size([5, 1, 57])\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    lang, name, lang_t, name_t = random_language_name()\n",
    "    print(\"language: \", lang,  lang_t, \"name: \", name, name_t.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the specific RNN structure that we want to be building:\n",
    "\n",
    "![network diagram](network_structure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#livecode\n",
    "class RNN(nn.Module):\n",
    "    \"\"\"This dictates the structure and size of the network\"\"\"\n",
    "    def __init__(self, data_size, hidden_size, output_size):\n",
    "        \"\"\"Sets up class attributes (mostly data dimensions) and network layers\"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        self.combined_input_size = data_size + hidden_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # fully connected layers\n",
    "        self.i2o = nn.Linear(self.combined_input_size, output_size) \n",
    "        self.i2h = nn.Linear(self.combined_input_size, hidden_size) \n",
    "        \n",
    "        #softmax to turn output into probability\n",
    "        self.softmax = nn.Softmax(dim=1) \n",
    "    \n",
    "    def forward(self, x, last_hidden):\n",
    "        \"\"\"Describes how data moves through the RNN layers\"\"\"\n",
    "        combined_input = torch.cat([x, last_hidden], dim=1)\n",
    "        output = self.i2o(combined_input)\n",
    "        hidden = self.i2h(combined_input)\n",
    "        output_probabilities = self.softmax(output)\n",
    "        return output_probabilities, hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise a RNN\n",
    "n_hidden = 128\n",
    "rnn = RNN(n_letters, n_hidden, n_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_start = torch.zeros(1,n_hidden)\n",
    "output, hidden = rnn(character_to_tensor('a'), hidden_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size() # (1, n_languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0606, 0.0527, 0.0531, 0.0550, 0.0611, 0.0497, 0.0554, 0.0563, 0.0506,\n",
       "         0.0627, 0.0497, 0.0513, 0.0573, 0.0546, 0.0542, 0.0606, 0.0563, 0.0586]],\n",
       "       grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.size() # (1, n_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### making sense of model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# live code\n",
    "def language_from_output(output):\n",
    "    \"\"\"takes output probability vector and returns top language\"\"\"\n",
    "    top_n, top_i = output.topk(1) # gives the top 1 score and index\n",
    "    index = top_i[0].item() # convert tensor to float\n",
    "    score = top_n[0].item()\n",
    "    language = all_languages[index]\n",
    "    return language, index, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('English', 9, 0.06270859390497208)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_from_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss() # negative log likelihood loss. Convention\n",
    "lr = 0.005 # starter learning rate\n",
    "\n",
    "def predict(rnn, name_t):\n",
    "    \"\"\"pass each character in an input name tensor through the network\"\"\"\n",
    "    hidden = torch.zeros(1,rnn.hidden_size) # initial zero hidden state\n",
    "    rnn.zero_grad() # removes all gradients from last time\n",
    "    # loops over the name, passing the hidden vector along each time\n",
    "    for i in range(name_t.size(0)):\n",
    "        output, hidden = rnn(name_t[i], hidden) \n",
    "    return output\n",
    "\n",
    "def train_step(lang_t, name_t):\n",
    "    \"\"\"predict language, compare with target, and update model parameters \"\"\"\n",
    "    \n",
    "    output = predict(rnn, name_t)\n",
    "    \n",
    "    log_likelihood = torch.log(output) # convert to log cause math\n",
    "    \n",
    "    # how wrong are we?\n",
    "    loss = criterion(log_likelihood, lang_t)\n",
    "    \n",
    "    # automagick backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-lr, p.grad.data)\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0551, 0.0603, 0.0561, 0.0571, 0.0577, 0.0523, 0.0534, 0.0553, 0.0571,\n",
       "          0.0606, 0.0589, 0.0512, 0.0512, 0.0588, 0.0568, 0.0552, 0.0526, 0.0503]],\n",
       "        grad_fn=<SoftmaxBackward>), 2.9514334201812744)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang, name, lang_t, name_t = random_language_name()\n",
    "train_step(lang_t, name_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_since(start):\n",
    "    \"\"\"little helper for pretty timestamping\"\"\"\n",
    "    now = time.time()\n",
    "    dt = now - start\n",
    "    mins = int(dt/60)\n",
    "    return f'{mins:>02}:{dt - mins*60:>05.2f}(m:s)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(rnn, n_iters):\n",
    "    \"\"\"loop the train_step and view progress\"\"\"\n",
    "    eval_every = (n_iters//10)\n",
    "    losses = []\n",
    "    start = time.time()\n",
    "    for i in range(n_iters):\n",
    "        lang, name, lang_t, name_t = random_language_name()\n",
    "        output, loss = train_step(lang_t, name_t)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if (i+1)%eval_every==0 :\n",
    "                av_loss = sum(losses[-eval_every:])/eval_every\n",
    "                print(f'iter = {i+1:<6} training time = {time_since(start)} %done = {i/n_iters*100:<5.0f}% loss = {av_loss:<10.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter = 1000   training time = 00:01.52(m:s) %done = 10   % loss = 2.8676    \n",
      "iter = 2000   training time = 00:02.91(m:s) %done = 20   % loss = 2.8124    \n",
      "iter = 3000   training time = 00:04.26(m:s) %done = 30   % loss = 2.7702    \n",
      "iter = 4000   training time = 00:05.61(m:s) %done = 40   % loss = 2.7229    \n",
      "iter = 5000   training time = 00:06.98(m:s) %done = 50   % loss = 2.5870    \n",
      "iter = 6000   training time = 00:08.33(m:s) %done = 60   % loss = 2.4306    \n",
      "iter = 7000   training time = 00:09.69(m:s) %done = 70   % loss = 2.3683    \n",
      "iter = 8000   training time = 00:11.10(m:s) %done = 80   % loss = 2.3278    \n",
      "iter = 9000   training time = 00:12.46(m:s) %done = 90   % loss = 2.2808    \n",
      "iter = 10000  training time = 00:13.89(m:s) %done = 100  % loss = 2.1941    \n"
     ]
    }
   ],
   "source": [
    "train(rnn, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure of success: Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# live code \n",
    "def language_confusion(rnn, language, n_samples = 100):\n",
    "    \"\"\"evaluate how often the model predicts names from a language correctly\"\"\"\n",
    "    language_predicted = {l:0 for l in all_languages}\n",
    "    for i in range(n_samples):\n",
    "        lang, name, lang_t, name_t = random_language_name(language=language)\n",
    "        output = infer(rnn, name_t)\n",
    "        guess_lang, guess_index, _ = language_from_output(output)\n",
    "        language_predicted[guess_lang]+=1\n",
    "        norm_predictions = {l:count/n_samples for l, count in language_predicted.items()}\n",
    "    return norm_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Czech': 0.06,\n",
       "  'German': 0.0,\n",
       "  'Arabic': 0.19,\n",
       "  'Japanese': 0.14,\n",
       "  'Chinese': 0.0,\n",
       "  'Vietnamese': 0.03,\n",
       "  'Russian': 0.01,\n",
       "  'French': 0.0,\n",
       "  'Irish': 0.0,\n",
       "  'English': 0.0,\n",
       "  'Spanish': 0.0,\n",
       "  'Greek': 0.0,\n",
       "  'Italian': 0.02,\n",
       "  'Portuguese': 0.0,\n",
       "  'Scottish': 0.02,\n",
       "  'Dutch': 0.04,\n",
       "  'Korean': 0.05,\n",
       "  'Polish': 0.44},\n",
       " 1.0000000000000002)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = language_confusion(rnn, \"Polish\")\n",
    "preds, sum([p for p in preds.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_total_confusion(rnn,samples_per_language=100):\n",
    "    \"\"\"Calculates confusion array for all languages\"\"\"\n",
    "    confusion_list = []\n",
    "    for target_lang in all_languages:\n",
    "        preds = language_confusion(rnn, target_lang, n_samples=samples_per_language)\n",
    "        predlist = np.array([[prob for lang, prob in preds.items()]])\n",
    "        confusion_list.append(predlist)\n",
    "    return np.concatenate(confusion_list, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion(confusion):\n",
    "    \"\"\"displays a nxn confusion numpy array\"\"\"\n",
    "    # Set up plot\n",
    "    fig = plt.figure(facecolor='white',figsize=(5,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # add the color scale\n",
    "    cax = ax.matshow(confusion)\n",
    "    cbar = fig.colorbar(cax)\n",
    "    \n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + all_languages, rotation=90)\n",
    "    ax.set_yticklabels([''] + all_languages)\n",
    "\n",
    "    # Force label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    \n",
    "    # labeling\n",
    "    ax.set_xlabel(\"predicted languag\")\n",
    "    ax.set_ylabel(\"target languag\")\n",
    "    cbar.ax.set_ylabel('predicted fraction')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = evaluate_total_confusion(samples_per_language = 100)\n",
    "plot_confusion(confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
